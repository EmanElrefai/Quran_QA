{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quran_QA Ensemble \n",
    "\n",
    "In last Notebook __[quran_qa_v1](https://github.com/EmanElrefai/Quran_QA/blob/main/quran_qa_v1.ipynb)__ I mentioned that there many difrent pretrained models have been tried with diffrent configurations parameters. I chose the models with higher results so that their pRR range is between [48.4 to 52.9]\n",
    "\n",
    "## The best models configuration.\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marwa\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Reading and spliting the data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import joblib\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_path = r'./datasets/qrcd_v1.1_dev.jsonl'\n",
    "\n",
    "def read_data(datapath):\n",
    "    \n",
    "    with open(datapath ,'rb') as fp:\n",
    "        datalist = list(fp)\n",
    "    data =[]\n",
    "    for json_str in datalist:\n",
    "        result = json.loads(json_str)\n",
    "        #print(f\"result: {result}\")\n",
    "        data.append(result)    \n",
    "    return data \n",
    "val_data=read_data(dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data spliting\n",
    "Where the data will splited to 8 parts. Each part has difrent 80% of data beacuse we have 8 tuning configurations for pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_multi_answers(data_list):\n",
    "    new_data=[]\n",
    "    #Loop all sampe\n",
    "    for sample in data_list :\n",
    "        # If the sample has a single answer append it to the new_data list and return back to pick the next sample.\n",
    "        if len(sample['answers']) ==1:\n",
    "            new_data.append(sample)\n",
    "            continue\n",
    "        # if the sample has multiple answers, Loop through all answeres of the sample, and store each answer with it passage, question in a single sample.\n",
    "        # And then append this sample to the new_data list.\n",
    "        for answer in sample['answers']:\n",
    "            new_sample={\n",
    "            'answers':[answer],\n",
    "            'passage':sample['passage'],\n",
    "            'pq_id':sample['pq_id'],\n",
    "            'question': sample['question'],\n",
    "            'surah':sample['surah'],\n",
    "            'verses':sample['verses']\n",
    "            }\n",
    "            new_data.append(new_sample)\n",
    "    return new_data \n",
    "\n",
    "# split the answers in the training and the validation datasets.\n",
    "val_data= split_multi_answers(val_data)\n",
    "val_datadf = pd.DataFrame(val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the AutoTokenizer which will be used to download the pretrained tokenizer model.\n",
    "token_transformer_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "\n",
    "# Downloading the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(token_transformer_name) # inputs id , attention mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_file(model_path, model_name):\n",
    "    model = joblib.load(model_path+model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars_run00.hd5\n",
      "stars_run01.hd5\n",
      "stars_run02.hd5\n",
      "stars_run03.hd5\n",
      "stars_run04.hd5\n",
      "stars_run05.hd5\n",
      "stars_run06.hd5\n",
      "stars_run07.hd5\n"
     ]
    }
   ],
   "source": [
    "models_path = './/models//'\n",
    "qa_models = []\n",
    "for model_name in os.listdir(models_path):\n",
    "    print(model_name)\n",
    "    \n",
    "    qa_model = read_model_file(models_path, model_name)\n",
    "    qa_models.append(qa_model)\n",
    "    \n",
    "#qa_model = joblib.load('.//models//stars_run12//stars_run12_model.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_quran_qav1(passage , question):\n",
    "    min_answer_length=1\n",
    "    number_of_required_answers = 5\n",
    "    models_outputs = []\n",
    "    ranked_answers=[]\n",
    "    \n",
    "    # Pass the question and the passage to the tokenizer\n",
    "    inputs = tokenizer(question, passage, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\") \n",
    "\n",
    "    # Obtain the input_ids from inputs\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    for model in qa_models:\n",
    "        # predict the inputs from the qa_model\n",
    "        models_outputs.append(model(**inputs))\n",
    "\n",
    "    \n",
    "    sum_start_answer_scores = models_outputs[0].start_logits\n",
    "    sum_end_answer_scores = models_outputs[0].end_logits\n",
    "    \n",
    "    for i in range(1, len(models_outputs)):\n",
    "        sum_start_answer_scores += models_outputs[i].start_logits\n",
    "        sum_end_answer_scores += models_outputs[i].end_logits\n",
    "    \n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    # answer_start = torch.argmax(answer_start_scores)\n",
    "    \n",
    "    answer_starts_probs = softmax(torch.topk(sum_start_answer_scores , 5).values.cpu().data.numpy())\n",
    "\n",
    "    #answer_starts_probs = softmax(torch.topk(sum_start_answer_scores , 5).values.cpu().detach().numpy())[0]\n",
    "    # print(answer_starts_probs)\n",
    "    answer_starts =  torch.topk(sum_start_answer_scores , 5).indices\n",
    "    \n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    # answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    answer_ends_probs = softmax(torch.topk(sum_end_answer_scores, 5).values.cpu().data.numpy())\n",
    "    #answer_ends_probs = softmax(torch.topk(sum_end_answer_scores, 5).values.cpu().detach().numpy())[0]\n",
    "    answer_ends = torch.topk(sum_end_answer_scores, 5).indices +1\n",
    "    \n",
    "    \n",
    "    # obtain the full probability by multiplying the matrix elementwise\n",
    "    full_probs = softmax((np.multiply(answer_starts_probs,answer_ends_probs)))[0] #check[1]\n",
    "\n",
    "    #print(f\"Question: {question}\")\n",
    "    #print('top predicted answers:')\n",
    "    idx =0\n",
    "    \n",
    "    # loop on each answer_start and answer_end indicies\n",
    "    #This loop mainly will be used to convert the indcies to the words according to the indcies obtained from above.\n",
    "    for answer_start ,  answer_end in zip(answer_starts.tolist()[0], answer_ends.tolist()[0]):\n",
    "        \n",
    "        idx+=1\n",
    "        # use the convert_tokens_to_string API to convert the input_ids\n",
    "        #'from the answer_start to the answer_end' back to the words starting from\n",
    "        answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "        \n",
    "       \n",
    "        #If the answer is not empty\n",
    "        if answer.strip() !='':\n",
    "            \n",
    "            # if the full_probs is good, then register the answer\n",
    "            if (full_probs[idx-1] > 0.1):    \n",
    "                \n",
    "                # print the answer\n",
    "                #print(f\"Answer number {idx}: {answer}\")\n",
    "                #Append the answer to the ranked_answers\n",
    "                ranked_answers.append( { 'answer': answer, 'rank' : len(ranked_answers)+1, 'score':float(full_probs[idx-1])})\n",
    "    #This is just for checking the ranked_answers if empty.\n",
    "    if len(ranked_answers) == 0:\n",
    "        print(' Empty Answer List')      \n",
    "    return ranked_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "def predict_quran_qa(passage , question):\n",
    "    min_answer_length=1\n",
    "    number_of_required_answers = 5\n",
    "    models_outputs = []\n",
    "    ranked_answers=[]\n",
    "    \n",
    "    # Pass the question and the passage to the tokenizer\n",
    "    inputs = tokenizer(question, passage, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\") \n",
    "\n",
    "    # Obtain the input_ids from inputs\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    for model in qa_models:\n",
    "        # predict the inputs from the qa_model\n",
    "        models_outputs.append(model(**inputs))\n",
    "\n",
    "    \n",
    "    sum_start_answer_scores = models_outputs[0].start_logits\n",
    "    sum_end_answer_scores = models_outputs[0].end_logits\n",
    "    \n",
    "    for i in range(1, len(models_outputs)):\n",
    "        sum_start_answer_scores += models_outputs[i].start_logits\n",
    "        sum_end_answer_scores += models_outputs[i].end_logits\n",
    "    \n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    # answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_starts_probs = softmax(torch.topk(sum_start_answer_scores , 5).values.cpu().detach().numpy())[0]\n",
    "    # print(answer_starts_probs)\n",
    "    answer_starts =  torch.topk(sum_start_answer_scores , 5).indices\n",
    "    \n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    # answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    answer_ends_probs = softmax(torch.topk(sum_end_answer_scores, 5).values.cpu().detach().numpy())[0]\n",
    "    answer_ends = torch.topk(sum_end_answer_scores, 5).indices +1\n",
    "    #print(f\"Question: {question}\")\n",
    "    idx =0\n",
    "    for i , answer_start in enumerate(answer_starts.tolist()[0]):\n",
    "        for j , answer_end in enumerate(answer_ends.tolist()[0]):\n",
    "            idx+=1\n",
    "            if (answer_end < answer_start or answer_end - answer_start + 1 < min_answer_length):\n",
    "                continue\n",
    "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "        \n",
    "            ranked_answers.append({ 'answer': answer, 'rank' : len(ranked_answers)+1,\n",
    "                                   'score':float(answer_starts_probs[i]*answer_ends_probs[j])})\n",
    "    #sort by probability \n",
    "    ranked_answers.sort(key =lambda x : x['score'], reverse =True)\n",
    "    ranked_answers = ranked_answers[:number_of_required_answers]\n",
    "    # reset rank \n",
    "    for i , answer in enumerate(ranked_answers):\n",
    "        answer['rank']=i+1\n",
    "        \n",
    "    if len(ranked_answers)==0: \n",
    "        raise BaseException(\"empty list \")\n",
    "    \n",
    "    #print('top predicted answer:',)\n",
    "    \n",
    "    return ranked_answers\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Pass the question and the passage to the tokenizer\\npassage= \\'سبحان الذي أسرى بعبده ليلا من المسجد الحرام إلى المسجد الأقصى الذي باركنا حوله لنريه من آياتنا إنه هو السميع البصير . واتينا موسى الكتاب وجعلناه هدى لبني إسرائيل أن لا تتخذوا من دوني وكيلا .\\'\\nquestion=\\'لماذا آسرى الله برسوله صلى الله عليه وسلم ؟\\'\\nmodels_outputs=[]\\ninputs = tokenizer(question, passage, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\") \\n\\n# Obtain the input_ids from inputs\\ninput_ids = inputs[\"input_ids\"].tolist()[0]\\n\\nfor model in qa_models:\\n    # predict the inputs from the qa_model\\n    models_outputs.append(model(**inputs))\\n\\nprint(models_outputs[0].start_logits.size())\\nprint(models_outputs[0].end_logits.size())\\n\\nprint(models_outputs[0].start_logits)   \\nprint(models_outputs[0].end_logits)\\n\\nprint()\\nprint(models_outputs[1].start_logits.size())\\nprint(models_outputs[1].end_logits.size())\\n\\nsum_start_answer = models_outputs[1].start_logits\\nsum_end_answer = models_outputs[1].end_logits\\nprint(models_outputs[1].start_logits)   \\nprint(models_outputs[1].end_logits)\\n\\n\\nprint()\\nprint(models_outputs[2].start_logits.size())\\nprint(models_outputs[2].end_logits.size())\\n\\nsum_start_answer = models_outputs[2].start_logits\\nsum_end_answer = models_outputs[2].end_logits\\nprint(models_outputs[2].start_logits)   \\nprint(models_outputs[2].end_logits)\\n\\nprint()\\nprint(models_outputs[3].start_logits.size())\\nprint(models_outputs[3].end_logits.size())\\n\\nsum_start_answer = models_outputs[3].start_logits\\nsum_end_answer = models_outputs[3].end_logits\\nprint(models_outputs[3].start_logits)   \\nprint(models_outputs[3].end_logits)\\n\\nprint()\\nprint(models_outputs[4].start_logits.size())\\nprint(models_outputs[4].end_logits.size())\\n\\nsum_start_answer = models_outputs[4].start_logits\\nsum_end_answer = models_outputs[4].end_logits\\nprint(models_outputs[4].start_logits)   \\nprint(models_outputs[4].end_logits)\\n\\nprint()\\nprint(models_outputs[5].start_logits.size())\\nprint(models_outputs[5].end_logits.size())\\n\\nsum_start_answer = models_outputs[5].start_logits\\nsum_end_answer = models_outputs[5].end_logits\\nprint(models_outputs[5].start_logits)   \\nprint(models_outputs[5].end_logits)\\n\\nprint()\\nprint(models_outputs[6].start_logits.size())\\nprint(models_outputs[6].end_logits.size())\\n\\nsum_start_answer = models_outputs[6].start_logits\\nsum_end_answer = models_outputs[6].end_logits\\nprint(models_outputs[6].start_logits)   \\nprint(models_outputs[6].end_logits)\\n\\nprint()\\nprint(models_outputs[7].start_logits.size())\\nprint(models_outputs[7].end_logits.size())\\n\\nsum_start_answer = models_outputs[7].start_logits\\nsum_end_answer = models_outputs[7].end_logits\\nprint(models_outputs[7].start_logits)   \\nprint(models_outputs[7].end_logits)\\n\\n\\nsum_start_answer = models_outputs[0].start_logits\\nsum_end_answer = models_outputs[0].end_logits\\n\\n\\nfor i in range(1, len(models_outputs)):\\n    #print(i)\\n    sum_start_answer += (models_outputs[i].start_logits)\\n    sum_end_answer += (models_outputs[i].end_logits)\\nprint(\\'summation \\n\\')    \\nprint(sum_start_answer)\\nprint(sum_end_answer)\\n\\nprint()\\nprint(torch.topk(sum_start_answer , 5))\\nprint(torch.topk(sum_end_answer , 5))\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Pass the question and the passage to the tokenizer\n",
    "passage= 'سبحان الذي أسرى بعبده ليلا من المسجد الحرام إلى المسجد الأقصى الذي باركنا حوله لنريه من آياتنا إنه هو السميع البصير . واتينا موسى الكتاب وجعلناه هدى لبني إسرائيل أن لا تتخذوا من دوني وكيلا .'\n",
    "question='لماذا آسرى الله برسوله صلى الله عليه وسلم ؟'\n",
    "models_outputs=[]\n",
    "inputs = tokenizer(question, passage, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\") \n",
    "\n",
    "# Obtain the input_ids from inputs\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "for model in qa_models:\n",
    "    # predict the inputs from the qa_model\n",
    "    models_outputs.append(model(**inputs))\n",
    "\n",
    "print(models_outputs[0].start_logits.size())\n",
    "print(models_outputs[0].end_logits.size())\n",
    "\n",
    "print(models_outputs[0].start_logits)   \n",
    "print(models_outputs[0].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[1].start_logits.size())\n",
    "print(models_outputs[1].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[1].start_logits\n",
    "sum_end_answer = models_outputs[1].end_logits\n",
    "print(models_outputs[1].start_logits)   \n",
    "print(models_outputs[1].end_logits)\n",
    "\n",
    "\n",
    "print()\n",
    "print(models_outputs[2].start_logits.size())\n",
    "print(models_outputs[2].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[2].start_logits\n",
    "sum_end_answer = models_outputs[2].end_logits\n",
    "print(models_outputs[2].start_logits)   \n",
    "print(models_outputs[2].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[3].start_logits.size())\n",
    "print(models_outputs[3].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[3].start_logits\n",
    "sum_end_answer = models_outputs[3].end_logits\n",
    "print(models_outputs[3].start_logits)   \n",
    "print(models_outputs[3].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[4].start_logits.size())\n",
    "print(models_outputs[4].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[4].start_logits\n",
    "sum_end_answer = models_outputs[4].end_logits\n",
    "print(models_outputs[4].start_logits)   \n",
    "print(models_outputs[4].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[5].start_logits.size())\n",
    "print(models_outputs[5].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[5].start_logits\n",
    "sum_end_answer = models_outputs[5].end_logits\n",
    "print(models_outputs[5].start_logits)   \n",
    "print(models_outputs[5].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[6].start_logits.size())\n",
    "print(models_outputs[6].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[6].start_logits\n",
    "sum_end_answer = models_outputs[6].end_logits\n",
    "print(models_outputs[6].start_logits)   \n",
    "print(models_outputs[6].end_logits)\n",
    "\n",
    "print()\n",
    "print(models_outputs[7].start_logits.size())\n",
    "print(models_outputs[7].end_logits.size())\n",
    "\n",
    "sum_start_answer = models_outputs[7].start_logits\n",
    "sum_end_answer = models_outputs[7].end_logits\n",
    "print(models_outputs[7].start_logits)   \n",
    "print(models_outputs[7].end_logits)\n",
    "\n",
    "\n",
    "sum_start_answer = models_outputs[0].start_logits\n",
    "sum_end_answer = models_outputs[0].end_logits\n",
    "\n",
    "\n",
    "for i in range(1, len(models_outputs)):\n",
    "    #print(i)\n",
    "    sum_start_answer += (models_outputs[i].start_logits)\n",
    "    sum_end_answer += (models_outputs[i].end_logits)\n",
    "print('summation \\n')    \n",
    "print(sum_start_answer)\n",
    "print(sum_end_answer)\n",
    "\n",
    "print()\n",
    "print(torch.topk(sum_start_answer , 5))\n",
    "print(torch.topk(sum_end_answer , 5))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result ={}\n",
    "for index, sample in val_datadf.iterrows():\n",
    "    result[sample['pq_id']] = predict_quran_qav1(sample['passage'], sample['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('stars_run100.json', 'w' , encoding= 'utf8') as fp:\n",
    "    json.dump(result , fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 109 records from ./datasets/qrcd_v1.1_dev.jsonl\n",
      "Warning: The answer scores of 9:60-61_316 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:60-61_316 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:60-61_316 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 2:178-179_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:178-179_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:190-194_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:243-245_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:74-76_400 at rank 1 and rank 2 are the same.\n",
      "Warning: The answer scores of 4:74-76_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:74-76_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:80-84_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:80-84_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:80-84_400 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 4:88-91_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 5:32-34_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 5:32-34_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 8:38-40_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:12-16_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:28-29_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:28-29_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:36-37_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:122-123_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:122-123_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 17:31-35_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 17:31-35_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 25:63-77_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 25:63-77_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:25-27_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:25-27_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:60-62_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:60-62_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:60-62_400 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 47:1-6_400 at rank 1 and rank 2 are the same.\n",
      "Warning: The answer scores of 47:1-6_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 49:9-10_400 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 49:9-10_400 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:34-39_257 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 7:19-25_257 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 7:19-25_257 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 7:19-25_257 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 20:115-123_257 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 20:115-123_257 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:83-86_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:178-179_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:228-230_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:234-237_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:234-237_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 3:145-148_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:122-126_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:122-126_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 5:12-13_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 5:12-13_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 5:82-86_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 5:82-86_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 7:54-56_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 7:54-56_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:100-102_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:100-102_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:117-121_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 17:22-30_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 18:29-31_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 18:29-31_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 18:29-31_313 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 22:30-37_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:28-31_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 39:9-10_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 39:32-37_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 41:33-36_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 41:33-36_313 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 46:15-18_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 53:31-41_313 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:124-129_103 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 10:65-70_149 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 10:65-70_149 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 20:86-94_163 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 20:86-94_163 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 20:86-94_163 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 20:95-98_163 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 6:161-163_373 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 6:161-163_373 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 39:11-20_373 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 39:11-20_373 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 39:11-20_373 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 104:1-9_107 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 104:1-9_107 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 41:13-18_236 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 46:21-25_236 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 51:38-46_236 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 51:38-46_236 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 69:1-8_236 at rank 1 and rank 2 are the same.\n",
      "Warning: The answer scores of 2:233-233_132 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:233-233_132 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 31:12-19_132 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 31:12-19_132 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 46:15-18_132 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 46:15-18_132 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 24:30-31_348 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 24:30-31_348 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:32-34_348 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:32-34_348 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:53-55_348 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:190-194_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:88-91_304 at rank 1 and rank 2 are the same.\n",
      "Warning: The answer scores of 4:88-91_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 8:62-66_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:1-6_304 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:73-74_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 25:45-55_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 47:1-6_304 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 47:1-6_304 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:151-153_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 3:30-32_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 3:30-32_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 3:130-136_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 3:130-136_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 3:164-165_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:12-14_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:12-14_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:58-59_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:58-59_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:64-65_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:66-70_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:66-70_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:80-84_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 4:80-84_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 4:80-84_415 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 5:90-93_415 at rank 1 and rank 2 are the same.\n",
      "Warning: The answer scores of 5:90-93_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 5:90-93_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 5:103-105_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 7:157-158_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 7:157-158_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 8:20-26_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:71-72_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 9:71-72_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 9:71-72_415 at rank 4 and rank 5 are the same.\n",
      "Warning: The answer scores of 24:46-54_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 24:46-54_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 24:55-57_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 24:55-57_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:21-24_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:36-40_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:36-40_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 33:69-71_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 33:69-71_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 47:33-38_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 47:33-38_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 48:16-17_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 49:6-8_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 49:6-8_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 58:12-13_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 59:6-7_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 59:6-7_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 64:11-13_415 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 64:11-13_415 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 2:256-257_417 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 2:256-257_417 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 10:107-109_417 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 17:12-17_417 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 17:12-17_417 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 18:29-31_417 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 18:29-31_417 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 27:89-93_417 at rank 2 and rank 3 are the same.\n",
      "Warning: The answer scores of 27:89-93_417 at rank 3 and rank 4 are the same.\n",
      "Warning: The answer scores of 39:38-41_417 at rank 2 and rank 3 are the same.\n",
      "The run file is correct.\n",
      "{\"pRR\": 0.6114219515081039, \"exact_match\": 0.30275229357798167, \"f1\": 0.5737629712427744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-29 02:02:53,006 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "!python ./evaluation/quranqa22_eval.py --gold_answers_file=./datasets/qrcd_v1.1_dev.jsonl --run_file=stars_run100.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train:\n",
      "[ 32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49\n",
      "  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127]\n",
      "\n",
      " test:\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n",
      "\n",
      " train:\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127]\n",
      "\n",
      " test:\n",
      "[32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55\n",
      " 56 57 58 59 60 61 62 63]\n",
      "\n",
      " train:\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127]\n",
      "\n",
      " test:\n",
      "[64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87\n",
      " 88 89 90 91 92 93 94 95]\n",
      "\n",
      " train:\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95]\n",
      "\n",
      " test:\n",
      "[ 96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
