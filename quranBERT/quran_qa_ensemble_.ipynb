{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHjzDs7yFMyd",
        "outputId": "b1cc165e-02f2-4ec0-b0e2-0ec86b6483ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, farasapy\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed farasapy-0.0.14 huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers farasapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdI045XSwgS9",
        "outputId": "bff6e90c-5854-4d08-db18-79f9399c7581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Quran_QA/models/quranBERTqa_4/checkpoint-2960\n",
            "/content/drive/MyDrive/Quran_QA/models/quranBERTqa_3/checkpoint-2480\n",
            "/content/drive/MyDrive/Quran_QA/models/quranBERTqa_3/checkpoint-4960\n",
            "/content/drive/MyDrive/Quran_QA/models/quranBERTqa_2/checkpoint-1480\n",
            "/content/drive/MyDrive/Quran_QA/models/quranBERTqa/checkpoint-2960\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "models_path =[\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_4/checkpoint-2960\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_4/checkpoint-1480\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_3/checkpoint-2480\",\n",
        "              \n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_3/checkpoint-4960\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_2/checkpoint-740\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_2/checkpoint-1480\",\n",
        "\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa4/checkpoint-1480\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa4/checkpoint-2960\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa/checkpoint-1480\" ,\n",
        "\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa/checkpoint-2960\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/2/checkpoint-9672\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/2/checkpoint-4836\" , \n",
        "\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/1quranqa4epoch\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/1quranqa2epoch8batch\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/1\",\n",
        "\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/1quranqa2epoch\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/2/checkpoint-2418\" ,\n",
        "              \"/content/drive/MyDrive/Quran_QA/models/miniquranqa\" , \n",
        "              \"/content/drive/MyDrive/Quran_QA/saved_model/0\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/saved_model/1\",\n",
        "              \"/content/drive/MyDrive/Quran_QA/saved_model/2\"\n",
        "\n",
        "]\n",
        "models =[]\n",
        "for path in models_path :\n",
        "  try :\n",
        "    models.append( AutoModelForQuestionAnswering.from_pretrained(path) )\n",
        "  except :\n",
        "    print(path)\n",
        "\n",
        "# for model in models : model.to(\"cuda\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdENqbfPGRJM",
        "outputId": "a2625a3f-af3b-4433-c615-eb40f4f6914d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rGHO1BBBo-T"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer , AutoTokenizer\n",
        "model_path= \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_2/checkpoint-740\"\n",
        "model_path= \"/content/drive/MyDrive/Quran_QA/models/quranBERTqa_3/checkpoint-4960\"\n",
        "model_path = \"aubmindlab/bert-base-arabertv02\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "# model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPhfA1kSoty4"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "from collections import Counter\n",
        "min_answer_length=3\n",
        "number_of_required_answers = 5\n",
        "def get_ids_from_logits(outputs):\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    # Get the most likely beginning of answer with the argmax of the score\n",
        "    # answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_starts_probs = softmax(torch.topk(answer_start_scores , 5).values.cpu().detach().numpy())[0]\n",
        "    # print(answer_starts_probs)\n",
        "    answer_starts =  torch.topk(answer_start_scores , 5).indices\n",
        "    # Get the most likely end of answer with the argmax of the score\n",
        "    # answer_end = torch.argmax(answer_end_scores) + 1\n",
        "    answer_ends_probs = softmax(torch.topk(answer_end_scores, 5).values.cpu().detach().numpy())[0]\n",
        "    answer_ends = torch.topk(answer_end_scores, 5).indices +1\n",
        "    \n",
        "    return answer_starts.tolist() , answer_ends.tolist() \n",
        "def quran_qa(text , question, show_all=False):\n",
        "    ranked_answers=[]\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")#.to(\"cuda\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    starts =[]\n",
        "    ends =[]\n",
        "    # print(inputs)\n",
        "    for model in models :\n",
        "      try :\n",
        "        outputs = model(**inputs) # predict two lists ( startsss , endsss ) argmax [3 , 4, 5 , 6 ,-4 , 3]\n",
        "        model_starts , model_ends = get_ids_from_logits(outputs)\n",
        "        # print(model_starts)\n",
        "        starts.extend(model_starts[0])\n",
        "        ends.extend(model_ends[0])\n",
        "      except BaseException as e :\n",
        "\n",
        "        print(\"failed ..\", e)\n",
        "    \n",
        "    starts =Counter(starts)\n",
        "    ends =Counter(ends)\n",
        "    # print(starts)\n",
        "    # print(ends)\n",
        "    answer_starts =list(starts.keys())[:number_of_required_answers]\n",
        "    answer_ends =list(ends.keys())[:number_of_required_answers]\n",
        "    answer_starts_probs = softmax([starts[x] for x in answer_starts])\n",
        "    answer_ends_probs = softmax([ends[x] for x in answer_ends ])\n",
        "    # print(outputs)\n",
        "    # print(f\"Question: {question}\")\n",
        "    idx =0\n",
        "    for i , answer_start in enumerate(answer_starts):\n",
        "     for j , answer_end in enumerate(answer_ends):\n",
        "        idx+=1\n",
        "        if (\n",
        "                answer_end < answer_start\n",
        "                or answer_end - answer_start + 1 < min_answer_length\n",
        "            ):\n",
        "                continue\n",
        "        answer = tokenizer.convert_tokens_to_string(\n",
        "            tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        "        )\n",
        "        \n",
        "        ranked_answers.append(\n",
        "            {\n",
        "                'answer': answer,\n",
        "                  'rank' : len(ranked_answers)+1,\n",
        "                  'score':float(answer_starts_probs[i]*answer_ends_probs[j])\n",
        "            }\n",
        "        )\n",
        "    #sort by probability \n",
        "    ranked_answers.sort(key =lambda x : x['score'], reverse =True)\n",
        "    ranked_answers = ranked_answers[:number_of_required_answers]\n",
        "    # reset rank \n",
        "    for i , answer in enumerate(ranked_answers): answer['rank']=i+1\n",
        "    if len(ranked_answers)==0: raise BaseException(\"empty list \")\n",
        "    # print('top predicted answer:', )\n",
        "    return ranked_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLuRh8iTKTQS",
        "outputId": "98252965-3938-459d-9751-aa4c126e6356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "failed .. index out of range in self\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'answer': 'ثلاث مائة سنين وازدادوا تسعا',\n",
              "  'rank': 1,\n",
              "  'score': 0.11372808073212853},\n",
              " {'answer': 'ثلاث مائة سنين', 'rank': 2, 'score': 0.11372808073212853},\n",
              " {'answer': 'مائة سنين وازدادوا تسعا',\n",
              "  'rank': 3,\n",
              "  'score': 0.11372808073212853},\n",
              " {'answer': 'مائة سنين', 'rank': 4, 'score': 0.11372808073212853},\n",
              " {'answer': 'ولبثوا في كهفهم ثلاث مائة سنين وازدادوا تسعا',\n",
              "  'rank': 5,\n",
              "  'score': 0.11372808073212853}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text =\"ولا تقولن لشئ إني فاعل ذلك غدا إلا أن يشاء الله . واذكر ربك إذا نسيت وقل عسى أن يهديني ربي لأقرب من هذا رشدا . ولبثوا في كهفهم ثلاث مائة سنين وازدادوا تسعا قل الله أعلم بما لبثوا له غيب السماوات والأرض ابصر به واسمع مالهم من دونه من ولي ولا يشرك في حكمه احدا .\"\n",
        "q=\"كم لبث اصحاب الكهف في الكهف ؟\"\n",
        "quran_qa(text ,q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YF3QcL_KavM"
      },
      "outputs": [],
      "source": [
        "train_data_path = r'/content/drive/MyDrive/Quran_QA/Q_QA_task_original_code/quranqa/datasets/qrcd_v1.1_train.jsonl'\n",
        "AQQAC_data = r'/content/drive/MyDrive/Quran_QA/data/AQQAC_less_384.jsonl'\n",
        "dev_data_path = r'/content/drive/MyDrive/Quran_QA/Q_QA_task_original_code/quranqa/datasets/qrcd_v1.1_dev.jsonl'\n",
        "\n",
        "import json \n",
        "def read_data(datapath):\n",
        "  with open(datapath ,'rb') as fp:\n",
        "      datalist = list(fp)\n",
        "  data =[]\n",
        "  for json_str in datalist:\n",
        "      result = json.loads(json_str)\n",
        "      # print(f\"result: {result}\")\n",
        "      data.append(result)\n",
        "  return data \n",
        "val_data = read_data(dev_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJT5V0gBNh2b"
      },
      "outputs": [],
      "source": [
        "testset_path =\"/content/drive/MyDrive/Quran_QA/Q_QA_task_original_code/quranqa/datasets/qrcd_v1.1_test_noAnswers.jsonl\"\n",
        "testset_data = read_data(testset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUiHhomRLqpJ"
      },
      "outputs": [],
      "source": [
        "result ={}\n",
        "for sample in testset_data :\n",
        "  result[sample['pq_id']]=quran_qa(sample['passage'], sample['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb8VMkLRLzE_"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "with open('star_run13.json', 'w' , encoding= 'utf8') as fp:\n",
        "  json.dump(result , fp, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs9rL8EKTldt"
      },
      "outputs": [],
      "source": [
        "## "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "quran qa ensemble .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
